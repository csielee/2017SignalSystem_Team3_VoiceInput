<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Google 語音辨識 API - demo1</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://code.highcharts.com/highcharts.js"></script>
  </head>
  <body>
    <div id="test"></div>
    <canvas id="showmusic" style="width:500px;height:250px;background-color:gray"></canvas>
    <script lang="javascript">
      window.onload = function(){
        window.AudioContext = window.AudioContext || window.webkitAudioContext;
        navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia;
        //var recorder;
      }
      var musicchart;
      var musicdata = [];
      musicchart = new Highcharts.Chart({//chart1就是這個圖表本身，可以在建立完成後操作
        chart: {
          renderTo: 'test',//目標的div
          type: 'line'//圖表的種類
        },
        title: {
          text: 'music'//標題
        },
        yAxis : {
          max:1,
          min:-1
        },
        series: [{//這裡是圖表的資料
          name: 'voice',
          data: [1, 0, 4]
        }]
      });
      musicchart.series[0].setData([0,1,2,3,4,5,6]);
      var canvas = document.getElementById('showmusic');
      var ctx = canvas.getContext('2d');
      if (!('webkitSpeechRecognition' in window)) {
        alert('no');
      } else {
        alert('yes');
        var start,end,elapsed;
        start = new Date();
        var recognition = new webkitSpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang="cmn-Hant-TW";

        var microphone,scriptNode;
        recognition.onstart=function(){
          console.log('開始辨識...');
          
          var context = new AudioContext();
          navigator.getUserMedia({audio: true}, function(stream) {
            microphone = context.createMediaStreamSource(stream);
            scriptNode = context.createScriptProcessor(4096, 1, 1);
            
            scriptNode.onaudioprocess = function(audioProcessingEvent) {
              // The input buffer is the song we loaded earlier
              var inputBuffer = audioProcessingEvent.inputBuffer;
              var outputBuffer = audioProcessingEvent.outputBuffer;
              // Loop through the output channels (in this case there is only one)
              for (var channel = 0; channel < outputBuffer.numberOfChannels; channel++) {
                //console.log(inputBuffer.getChannelData(channel));
                musicdata.push(inputBuffer.getChannelData(channel))
                musicchart.series[0].setData(musicdata);
              }
            }

            microphone.connect(scriptNode);
            scriptNode.connect(context.destination);
          }, function(){
            console.log('error');
          });
        };
        recognition.onend=function(){
          console.log('停止辨識!');
          microphone.disconnect();
          scriptNode.disconnect();
        };

        recognition.onresult=function(event){
          console.log(event);
          //end = new Date();
          //elapsed = end.getTime() - start.getTime();
          //console.log(elapsed);
          //recognition.end();
          var str = event.results[event.resultIndex][0].transcript;
          console.log(str);
          if (str.match("停")!=null)
            recognition.stop();
        };

        recognition.start();
      }
    </script>
  </body>
</html>